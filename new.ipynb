{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd02b9cd5c3ec4eb793e30b3fe22fb3034f3d8c9722bad10f4cedbbd7faa7e7f261",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_url(url):\n",
    "\n",
    "    if url.find(\"www.amazon.in\") != -1:\n",
    "        index = url.find(\"/dp/\")\n",
    "        if index != -1:\n",
    "            index2 = index + 14\n",
    "            url = \"https://www.amazon.in\" + url[index:index2]\n",
    "        else:\n",
    "            index = url.find(\"/gp/\")\n",
    "            if index != -1:\n",
    "                index2 = index + 22\n",
    "                url = \"https://www.amazon.in\" + url[index:index2]\n",
    "            else:\n",
    "                url = None\n",
    "    else:\n",
    "        url = None\n",
    "    return url\n",
    "\n",
    "def get_converted_price(price):\n",
    "    converted_price = float(re.sub(r\"[^\\d.]\", \"\", price))\n",
    "    return converted_price\n",
    "\n",
    "def get_page_soup(clean_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0\"\n",
    "    }\n",
    "    page = requests.get(clean_url, headers=headers)\n",
    "    return BeautifulSoup(page.content, \"html5lib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_amazon_data(url):\n",
    "    \n",
    "    details = {\"name\": \"\", \"price\": 0, \"deal\": True, \"url\": \"\",'website':'amazon'}\n",
    "    _url = extract_url(url)\n",
    "    soup = get_page_soup(_url)\n",
    "    title = soup.find(id=\"productTitle\")\n",
    "    price = soup.find(id=\"priceblock_dealprice\")\n",
    "    if price is None:\n",
    "        price = soup.find(id=\"priceblock_ourprice\")\n",
    "        details[\"deal\"] = False\n",
    "    if title is not None and price is not None:\n",
    "        details[\"name\"] = title.get_text().strip()\n",
    "        details[\"price\"] = get_converted_price(price.get_text())\n",
    "        details[\"url\"] = _url\n",
    "    return details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  extract_myntra_data(url):\n",
    "    details = {\"name\": \"\", \"price\": 0, \"deal\": False, \"url\": \"\",'website':'myntra'}\n",
    "    headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}\n",
    "    s = requests.Session()\n",
    "    res = s.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text,\"lxml\")\n",
    "    script = None\n",
    "    # for s in soup.find_all(\"script\"):\n",
    "    #     if 'pdpData' in s.text:\n",
    "    #         script = s.get_text(strip=True)\n",
    "    #         print(script)\n",
    "    #         break\n",
    "    # datadict = json.loads(script[script.index('{'):])\n",
    "    # if datadict['pdpData'].get('price').get('discounted'):\n",
    "    #     details['price'] = datadict['pdpData'].get('price').get('discounted') \n",
    "    #     details['deal'] = True\n",
    "    # else:\n",
    "    #     details['price'] =datadict['pdpData'].get('price').get('mrp')\n",
    "    # details['name'] = datadict['pdpData'].get('name')\n",
    "    # details['url'] = url.split('?')[0]\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flipkart_data(url):\n",
    "    details = {\"name\": \"\", \"price\": 0, \"deal\": False, \"url\": \"\",'website':'flipkart'}\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:68.0) Gecko/20100101 Firefox/68.0\"\n",
    "    }\n",
    "    page = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(page.content, \"html5lib\")\n",
    "    \n",
    "        \n",
    "    try:\n",
    "        title = soup.find('h1',{'class':'yhB1nd'}).span\n",
    "        price = soup.find('div',{'class':'_30jeq3 _16Jk6d'})\n",
    "        offer_text = soup.find('div',{'class':'_1V_ZGU'}).span.text\n",
    "        if 'special' in offer_text.lower():\n",
    "            details[\"deal\"] = True\n",
    "    except:\n",
    "        details['deal'] = False\n",
    "    if title is not None and price is not None:\n",
    "        details[\"name\"] = title.get_text().strip()\n",
    "        details[\"price\"] = get_converted_price(price.get_text())\n",
    "        details[\"url\"] = url.split('?')[0]\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = extract_myntra_data('https://www.myntra.com/sunglasses/hrx-by-hrithik-roshan/hrx-by-hrithik-roshan-men-square-sunglasses-mfb-pn-cy-80249/2311895/buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"
     ]
    }
   ],
   "source": [
    "for script in soup.find_all('script'):\n",
    "    print(script.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}